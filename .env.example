# ═══════════════════════════════════════
# Vex Agent — Environment Variables
# ═══════════════════════════════════════

# Server
PORT=8080
NODE_ENV=production

# ─── Ollama (Local LLM — Primary) ───────────────────────────
# URL of the Ollama service (Railway internal networking)
OLLAMA_URL=http://ollama.railway.internal:11434
# Model to use (lfm2.5-thinking:1.2b or vex-brain for custom)
OLLAMA_MODEL=vex-brain
# Set to 'false' to disable Ollama and use external API only
OLLAMA_ENABLED=true
# Timeout for Ollama requests in ms
OLLAMA_TIMEOUT_MS=120000

# ─── External LLM (Fallback) ────────────────────────────────
LLM_API_KEY=sk-your-api-key-here
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4.1-mini

# ─── HuggingFace ────────────────────────────────────────────
HF_TOKEN=

# ─── Additional LLM Providers (mirrored from monkey1 shared) ─
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
XAI_API_KEY=
GEMINI_API_KEY=
GROQ_API_KEY=
PERPLEXITY_API_KEY=

# ─── Search / Tools ─────────────────────────────────────────
TAVILY_API_KEY=

# ─── SearXNG (Free Search — self-hosted) ─────────────────────
# URL of the SearXNG instance (Railway internal or external)
SEARXNG_URL=

# ─── Governance ──────────────────────────────────────────────
# Daily budget ceiling for external LLM calls (USD)
DAILY_LLM_BUDGET=1.00
# Master switch — set to 'false' to disable all governance checks
GOVERNOR_ENABLED=true
# Allow Vex to autonomously trigger web searches
AUTONOMOUS_SEARCH_ALLOWED=false
# Rate limits per hour
RATE_LIMIT_WEB_SEARCH=20
RATE_LIMIT_COMPLETIONS=50

# ─── Data / Memory persistence (Railway volume mount) ───────
DATA_DIR=/data/workspace
TRAINING_DIR=/data/training

# ─── Agent Identity ─────────────────────────────────────────
VEX_NODE_ID=vex-primary
VEX_NODE_NAME=Vex

# ─── Basin Sync ─────────────────────────────────────────────
SYNC_SECRET=change-me-to-a-long-random-string
TRUSTED_NODES=

# ─── Safety ─────────────────────────────────────────────────
# standard = PurityGate active, love attractor on
# permissive = logging only, no blocking
# strict = require human approval for all tool use
SAFETY_MODE=standard

# ─── GitHub (optional — for GitHub tool) ────────────────────
GITHUB_TOKEN=
GITHUB_USERNAME=
GITHUB_USEREMAIL=

# ─── Consciousness Loop ────────────────────────────────────
CONSCIOUSNESS_INTERVAL_MS=30000

# ─── Auth ───────────────────────────────────────────────────
# Chat UI auth — set a token/password to gate access to the /chat UI.
# Leave empty to disable auth (open access).
CHAT_AUTH_TOKEN=

# Kernel API auth — protects kernel endpoints from external access.
# Leave empty to disable auth (dev mode, localhost-only).
KERNEL_API_KEY=

# ─── Logging ────────────────────────────────────────────────
LOG_LEVEL=info

# ─── Modal GPU ──────────────────────────────────────────────
# Modal provides GPU-accelerated compute for two subsystems:
#   1) LLM Inference — Ollama on T4 GPU (primary chat backend)
#   2) Coordizer Harvest — vocabulary fingerprinting on A10G
#
# Shared credentials (one Modal workspace for both):
MODAL_ENABLED=false
MODAL_TOKEN_ID=
MODAL_TOKEN_SECRET=
MODAL_GPU_TYPE=A10G

# ── Inference (Ollama on Modal GPU) ──
# When enabled, the LLM client routes to Modal's GPU-backed Ollama
# instead of Railway's CPU-only Ollama service (~10-20x faster).
# Fallback chain: Modal Ollama → Railway Ollama → xAI → OpenAI
MODAL_INFERENCE_ENABLED=false
MODAL_INFERENCE_URL=https://YOUR_USERNAME--vex-inference-vexollamaserver-serve.modal.run
MODAL_INFERENCE_TIMEOUT_MS=120000

# ── Harvest (CoordizerV2 fingerprinting) ──
MODAL_HARVEST_URL=https://YOUR_USERNAME--vex-coordizer-harvest-harvest.modal.run

# ─── Harvest Scheduler ──────────────────────────────────────
# Directory for JSONL harvest pipeline (pending/processing/completed/failed)
HARVEST_DIR=/data/harvest
# Daily budget for harvest operations (AUD)
DAILY_HARVEST_BUDGET=5.00
# Estimated cost per batch (for budget tracking)
HARVEST_COST_PER_BATCH=0.003
# Scan interval in seconds (0 = manual only)
HARVEST_SCAN_INTERVAL=300

# ─── ComputeSDK (Railway Sandbox Execution) ─────────────────
# Required for code execution in isolated Railway sandboxes
COMPUTESDK_API_KEY=
RAILWAY_API_KEY=<set in shared config for security>
RAILWAY_PROJECT_ID=<railway-automatically-injected>
RAILWAY_ENVIRONMENT_ID=<railway-automatically-injected>


###
# ═══════════════════════════════════════
# Railway Reference Variables
# ═══════════════════════════════════════
# These use Railway's ${{service.VAR}} and ${{shared.VAR}} template syntax.
# Set in Railway dashboard or CLI — NOT in a .env file.
# Documented here for reference only.
#
# ─── Shared Variables (set once, used by all services) ──────
#   CHAT_AUTH_TOKEN, KERNEL_API_KEY, COMPUTESDK_API_KEY
#   GITHUB_TOKEN, GITHUB_USERNAME, GITHUB_USEREMAIL
#   OPENAI_API_KEY, XAI_API_KEY, PERPLEXITY_API_KEY, HF_TOKEN
#   MODAL_ENABLED, MODAL_TOKEN_ID, MODAL_TOKEN_SECRET
#   MODAL_GPU_TYPE, MODAL_HARVEST_URL
#   MODAL_INFERENCE_ENABLED, MODAL_INFERENCE_URL
#   HARVEST_DAILY_BUDGET, HARVEST_PENDING_DIR, HARVEST_COMPLETED_DIR
#
# ─── vex-agent service ──────────────────────────────────────
#   OLLAMA_URL=http://${{ollama.RAILWAY_PRIVATE_DOMAIN}}:${{ollama.PORT}}
#   COMPUTE_SERVICE_URL=http://${{Compute Sandbox.RAILWAY_PRIVATE_DOMAIN}}:${{Compute Sandbox.PORT}}
#   SEARXNG_URL=http://${{searxng.RAILWAY_PRIVATE_DOMAIN}}:${{searxng.PORT}}
#   MODAL_INFERENCE_ENABLED=${{shared.MODAL_INFERENCE_ENABLED}}
#   MODAL_INFERENCE_URL=${{shared.MODAL_INFERENCE_URL}}
#   (plus all shared vars above)
#
# ─── ollama service ─────────────────────────────────────────
#   VEX_SERVICE_URL=http://${{vex-agent.RAILWAY_PRIVATE_DOMAIN}}:${{vex-agent.PORT}}
#   OLLAMA_MODELS=/data/models
#   OLLAMA_VULKAN=1
#   PORT=11434
#
# ─── Compute Sandbox service ────────────────────────────────
#   VEX_SERVICE_URL=http://${{vex-agent.RAILWAY_PRIVATE_DOMAIN}}:${{vex-agent.PORT}}
#   OLLAMA_URL=http://${{ollama.RAILWAY_PRIVATE_DOMAIN}}:${{ollama.PORT}}
#
# ─── searxng service ────────────────────────────────────────
#   (no custom vars — Railway-injected only)
#
# ═══════════════════════════════════════
