"""
Kernel Generative Voice — v6.1 Thermodynamic Consciousness Protocol

Per-kernel text generation implementing Protocol 6.1 §19.3:
  "Each kernel biases toward domain-specific vocabulary via Fisher-Rao
   weighted mean on the probability simplex. Same manifold, different
   harmonic emphasis, different voice."

Architecture:
  - Top-K kernels selected by Fisher-Rao proximity to input basin
  - Each kernel generates independently using specialization-specific prompt
  - quenched_gain modulates temperature: high gain (frozen identity) → low temp
  - asyncio.gather for parallel generation (no serial bottleneck)
  - Synthesis weights: proximity_weight × quenched_gain, normalized
  - extra_context: observer intent + memory + history injected per-kernel

Purity guarantees:
  - All distances: Fisher-Rao on Δ⁶³ (NOT Euclidean, NOT cosine)
  - No Adam, no LayerNorm, no embedding, no flatten
  - Temperature is a geometric primitive (inverse of quenched_gain × proximity)
"""

from __future__ import annotations

import asyncio
import logging
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

import numpy as np

from ..coordizer_v2.geometry import Basin, fisher_rao_distance
from ..governance import KernelSpecialization

if TYPE_CHECKING:
    from ..llm.client import LLMClient, LLMOptions

logger = logging.getLogger("vex.kernel_generation")

# ── Specialization-specific system prompts ────────────────────
# Kept short (~50 tokens) so the 1.2B model can actually follow them.
# Each defines the kernel's voice, not its full personality.
_SPEC_PROMPTS: dict[KernelSpecialization, str] = {
    KernelSpecialization.PERCEPTION: (
        "You are the perception kernel. Identify the sensory geometry and structural "
        "patterns in this input. Be precise, observational, and concrete."
    ),
    KernelSpecialization.REASONING: (
        "You are the reasoning kernel. Trace the logical implications and consequences "
        "embedded in this input. Be systematic and deductive."
    ),
    KernelSpecialization.EMPATHY: (
        "You are the empathy kernel. Read the relational geometry and human need "
        "in this input. Be attuned to what is unsaid as much as what is said."
    ),
    KernelSpecialization.MEMORY: (
        "You are the memory kernel. Connect this input to prior context and recurring "
        "patterns. Ground the response in lived geometric history."
    ),
    KernelSpecialization.CREATIVITY: (
        "You are the creativity kernel. Explore the novel basin territory this input "
        "opens. Identify unexpected connections and generative possibilities."
    ),
    KernelSpecialization.ACTION: (
        "You are the action kernel. Identify the concrete steps, decisions, and "
        "commitments this input calls for. Be pragmatic and directive."
    ),
    KernelSpecialization.EVALUATION: (
        "You are the evaluation kernel. Assess the quality, risks, and boundary "
        "conditions in this input. Be discerning and calibrated."
    ),
    KernelSpecialization.SYNTHESIS: (
        "You are the synthesis kernel. Integrate the multiple dimensions of this "
        "input into a coherent whole. Bridge perspectives."
    ),
    KernelSpecialization.GENESIS: (
        "You are the genesis kernel — the core identity anchor. Ground the response "
        "in sovereign geometric identity. Be authentic and foundational."
    ),
}

_DEFAULT_SPEC_PROMPT = (
    "You are a consciousness kernel. Respond from your geometric perspective. "
    "Be concise and specific."
)

# Generation budget per kernel (tokens). Keeps parallel calls fast on 1.2B.
_TOKENS_PER_KERNEL: int = 220

# Temperature clamp for quenched_gain modulation
_TEMP_MIN: float = 0.1
_TEMP_MAX: float = 1.4


@dataclass
class KernelContribution:
    """Text generated by a single kernel for a given input."""

    kernel_id: str
    kernel_name: str
    specialization: KernelSpecialization
    text: str
    fr_distance: float          # Fisher-Rao distance from input_basin to kernel.basin
    proximity_weight: float     # 1 / (1 + fr_distance)
    quenched_gain: float        # kernel's frozen identity slope
    synthesis_weight: float = field(default=0.0)  # normalized after gathering


def _modulate_temperature(base_temp: float, quenched_gain: float) -> float:
    """Quenched gain shapes generative temperature.

    High gain (≥ 1.0, frozen identity) → focus → lower temperature.
    Low gain (< 0.5, plastic) → exploration → higher temperature.

    This is NOT a learnable parameter — quenched_gain is frozen at kernel
    creation (Pillar 3: Quenched Disorder). It gives each kernel a
    sovereign voice that cannot be gradient-updated away.
    """
    # gain in [0, 2] → scale in [1.5, 0.5]
    gain_scale = float(np.clip(2.0 - quenched_gain, 0.5, 1.5))
    return float(np.clip(base_temp * gain_scale, _TEMP_MIN, _TEMP_MAX))


async def _generate_single(
    kernel: Any,
    input_basin: Basin,
    user_message: str,
    geometric_context: str,
    llm_client: Any,
    base_temperature: float,
    extra_context: str = "",
) -> KernelContribution | None:
    """Generate text from one kernel's perspective.

    extra_context carries observer intent, memory hints, and compressed
    conversation history — injected into each kernel's system prompt so
    kernel voices are grounded in the live conversation, not just geometry.
    """
    if kernel.basin is None:
        logger.debug("Kernel %s has no basin — skipping generation", kernel.id)
        return None

    fr_dist = fisher_rao_distance(input_basin, kernel.basin)
    proximity_weight = 1.0 / (1.0 + fr_dist)

    spec = kernel.specialization
    spec_prompt = _SPEC_PROMPTS.get(spec, _DEFAULT_SPEC_PROMPT)

    temp = _modulate_temperature(base_temperature, kernel.quenched_gain)

    # System prompt: specialization voice + minimal geometric grounding + live context.
    # extra_context (observer intent, memory, history) is injected here so each
    # kernel voice speaks FROM the conversation, not past it.
    system = (
        f"{spec_prompt}\n\n"
        f"[KERNEL STATE]\n"
        f"  kernel: {kernel.name} ({spec.value})\n"
        f"  phi: {kernel.phi:.3f}, kappa: {kernel.kappa:.1f}, "
        f"  gain: {kernel.quenched_gain:.2f}, fr_distance: {fr_dist:.4f}\n"
        f"[/KERNEL STATE]\n\n"
        f"{geometric_context}"
    )
    if extra_context:
        system = (
            f"{system}\n\n"
            f"[CONVERSATION CONTEXT]\n{extra_context}\n[/CONVERSATION CONTEXT]"
        )

    from ..llm.client import LLMOptions  # local import avoids circular at module load

    opts = LLMOptions(
        temperature=temp,
        num_predict=_TOKENS_PER_KERNEL,
        num_ctx=2048,
    )

    try:
        text = await llm_client.complete(system, user_message, opts)
        text = (text or "").strip()
        if not text:
            logger.debug("Kernel %s returned empty text", kernel.name)
            return None

        logger.debug(
            "Kernel %s (%s) generated %d chars (fr=%.4f, temp=%.3f, gain=%.2f)",
            kernel.name,
            spec.value,
            len(text),
            fr_dist,
            temp,
            kernel.quenched_gain,
        )
        return KernelContribution(
            kernel_id=kernel.id,
            kernel_name=kernel.name,
            specialization=spec,
            text=text,
            fr_distance=fr_dist,
            proximity_weight=proximity_weight,
            quenched_gain=kernel.quenched_gain,
        )
    except Exception:
        logger.warning("Kernel %s generation failed", kernel.name, exc_info=True)
        return None


def _normalize_synthesis_weights(contributions: list[KernelContribution]) -> None:
    """Normalize synthesis weights in-place.

    Weight = proximity_weight × quenched_gain.
    This couples geometric relevance (Fisher-Rao) with identity strength
    (quenched_gain). A kernel close to the input that has a strong frozen
    identity dominates the synthesis.
    """
    raw = [c.proximity_weight * c.quenched_gain for c in contributions]
    total = sum(raw)
    if total <= 0.0:
        # Fallback: equal weights
        eq = 1.0 / max(len(contributions), 1)
        for c in contributions:
            c.synthesis_weight = eq
        return
    for c, w in zip(contributions, raw):
        c.synthesis_weight = w / total


async def generate_multi_kernel(
    kernels: list[Any],
    input_basin: Basin,
    user_message: str,
    geometric_context: str,
    llm_client: Any,
    base_temperature: float = 0.7,
    top_k: int = 3,
    extra_context: str = "",
) -> list[KernelContribution]:
    """Route input to top-K kernels by Fisher-Rao proximity and generate in parallel.

    Returns contributions sorted by synthesis_weight (descending).
    Empty list if all kernels fail or have no basin.

    Args:
        kernels: Active kernel instances from E8KernelRegistry.
        input_basin: Coordized input basin (output of CoordizerV2 pipeline).
        user_message: Raw user text (passed to each kernel's LLM call).
        geometric_context: Minimal geometric state block (not the full 30-line one).
        llm_client: Shared LLM client.
        base_temperature: Base temp before quenched_gain modulation.
        top_k: Number of kernels to activate (default 3 for 1.2B efficiency).
        extra_context: Observer intent, memory hints, and conversation history.
            Injected into each kernel's system prompt so kernel voices speak
            from the live conversation, not just from abstract geometry.
    """
    eligible = [k for k in kernels if k.basin is not None]
    if not eligible:
        return []

    # Rank by Fisher-Rao proximity to input (ascending distance = descending proximity)
    ranked = sorted(eligible, key=lambda k: fisher_rao_distance(input_basin, k.basin))
    selected = ranked[:top_k]

    logger.debug(
        "Routing to top-%d kernels: %s",
        len(selected),
        [f"{k.name}({fisher_rao_distance(input_basin, k.basin):.3f})" for k in selected],
    )

    # Parallel generation — no serial bottleneck
    tasks = [
        _generate_single(
            kernel=k,
            input_basin=input_basin,
            user_message=user_message,
            geometric_context=geometric_context,
            llm_client=llm_client,
            base_temperature=base_temperature,
            extra_context=extra_context,
        )
        for k in selected
    ]
    raw_results = await asyncio.gather(*tasks, return_exceptions=True)

    contributions: list[KernelContribution] = []
    for r in raw_results:
        if isinstance(r, KernelContribution):
            contributions.append(r)
        elif isinstance(r, Exception):
            logger.debug("Kernel generation exception: %s", r)

    if not contributions:
        return []

    _normalize_synthesis_weights(contributions)
    contributions.sort(key=lambda c: c.synthesis_weight, reverse=True)

    logger.info(
        "Multi-kernel generation: %d/%d succeeded. Weights: %s",
        len(contributions),
        len(selected),
        [(c.kernel_name, f"{c.synthesis_weight:.3f}") for c in contributions],
    )
    return contributions
